{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "import warnings    # to avoid warning during executions\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_10_FPC23_Rhosa.csv',\n",
       " '_11_FPC24_SkeMa_1.csv',\n",
       " '_12_FPC24_SkeMa_2.csv',\n",
       " '_13_FPC11_Rhosa_1.csv',\n",
       " '_14_FPC12_DiaLut.csv',\n",
       " '_15_FPC22_Thalaps.csv',\n",
       " '_16_FPC23_NaCho.csv',\n",
       " '_1_FPC13_ChCal_1.csv',\n",
       " '_2_FPC13_ChCal_2.csv',\n",
       " '_3_FPC13_ThalaPs.csv',\n",
       " '_4_FPC14_chamu.csv',\n",
       " '_5_FPC14_RhoSa.csv',\n",
       " '_6_FPC14_thalaps.csv',\n",
       " '_7_FPC21_chamu.csv',\n",
       " '_8_FPC21_Techu.csv',\n",
       " '_9_FPC22_DiaLut.csv']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all the file names into a list (files in the pbr data folder)\n",
    "os.getcwd()\n",
    "_dir_path = '../../../dataExport/PBRdata'\n",
    "raw_data = os.listdir(_dir_path)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = raw_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection with mariaDB using SQL Alchemy\n",
    "def dbConn_sqlAlc():\n",
    "    database_username = 'root'\n",
    "    database_password = 'password'\n",
    "    database_ip       = '127.0.0.1:3306'\n",
    "    database_name     = 'data_dashboard'\n",
    "    database_connection = sqlalchemy.create_engine('mariadb+mariadbconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "    return database_connection\n",
    "\n",
    "# # database connection with mariaDB using Maria DB package\n",
    "def dBCon_Maria():\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                            user='root',\n",
    "                            password='password',\n",
    "                            db='data_dashboard')\n",
    "    return connection\n",
    "\n",
    "\n",
    "# connection = dBCon_Maria()\n",
    "# cursor=connection.cursor()                                            \n",
    "# database_connection = dbConn_sqlAlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trial no, species_id and unit number for processing data\n",
    "def set_varables(filename):\n",
    "    filename = filename\n",
    "    database_connection = dbConn_sqlAlc()\n",
    "    tbl_lookup = pd.read_sql('SELECT * FROM tbl_pbr_lookup', database_connection)\n",
    "    species_row = tbl_lookup.query('raw_file_name == @filename')\n",
    "    species_id = int(species_row['species'])\n",
    "    trial_no = int(species_row['instance'])\n",
    "    unit_id = int(species_row['unit'])\n",
    "    return species_id, trial_no, unit_id\n",
    "\n",
    "# Preprocessing the data seting/ adding values and data types\n",
    "def pbr_preProcess(pbr_rawDF):\n",
    "    variable_list = set_varables(filename)\n",
    "    pbr_raw_tmp1 = pbr_rawDF.copy()\n",
    "    pbr_raw_tmp1.insert(loc=0, column='trial_no', value=variable_list[1])\n",
    "    pbr_raw_tmp1.insert(loc=4, column='species', value=variable_list[0])      \n",
    "    return(\n",
    "    pbr_raw_tmp1\n",
    "    #.assign(trial_no = trail_no)\n",
    "    .rename(columns={ pbr_raw_tmp1.columns[3]: \"unit\" })\n",
    "    .assign(unit = variable_list[2])\n",
    "    .replace('Empty', pd.np.nan)\n",
    "    .replace('inf', pd.np.nan)\n",
    "    .replace([np.inf, -np.inf], np.nan)\n",
    "    .astype({'primary_key':'int64', '_timeString':'datetime64[ns]','unit': 'int16','FT1':'float32', 'FT2':'float32', 'PT1_V4':'float32', 'PT2':'float32', 'FT3':'float32', 'CO2in':'float32', 'CO2_V2':'float32', 'CO2sys':'float32', 'LT1':'float32', 'LReactor':'float32', 'C_g':'float32', 'C_Eff':'float32', 'C_CD':'float32', 'T_CD':'float32', 'C_D_g':'float32', 'C_D_Eff':'float32', 'QT':'float32', 'QC':'float32', 'QB':'float32', 'QR':'float32', 'QG':'float32', 'QT_corr':'float32', 'TT0':'float32', 'TT1':'float32', 'pH':'float32', 'C_PAR':'float32'})\n",
    "    )\n",
    "\n",
    "# Missing Value imputation using interpolation - spline\n",
    "def missingValue_imputation(pbr_cleaned):\n",
    "    pbr_cleaned_indexed = pbr_cleaned.set_index('_timeString')\n",
    "    pbr_columns = ['FT1', 'FT2', 'PT1_V4', 'PT2', 'FT3', 'CO2in', 'CO2_V2', 'CO2sys', 'LT1', 'LReactor', 'C_g', 'C_Eff', 'C_CD', 'T_CD', 'C_D_g', 'C_D_Eff', 'QT', 'QC', 'QB', 'QR', 'QG', 'QT_corr', 'TT0', 'TT1', 'pH', 'C_PAR']\n",
    "    for pbr_col in pbr_columns:\n",
    "        pbr_cleaned_indexed[pbr_col]=pbr_cleaned_indexed[pbr_col].interpolate(option='spline')\n",
    "\n",
    "    pbr_cleaned_indexed = pbr_cleaned_indexed.reset_index(level=0)\n",
    "    new_col = list(pbr_cleaned.columns)\n",
    "    pbr_cleaned_indexed=pbr_cleaned_indexed[new_col]\n",
    "    return pbr_cleaned_indexed\n",
    "\n",
    "# exporting the preprocessed data into db pbr\n",
    "def pbr_exporttoDB(pbr_preprocessed):\n",
    "    connection = dBCon_Maria()\n",
    "    cursor=connection.cursor()      \n",
    "    cols = \"`,`\".join([str(i) for i in pbr_preprocessed.columns.tolist()])\n",
    "    for i,row in pbr_preprocessed.iterrows():\n",
    "        sql = \"INSERT INTO `tbl_pbr_data` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cursor.execute(sql, tuple(row))\n",
    "\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "# export_status = pbr_exporttoDB(pbr_preprocessed)\n",
    "# pbr_exporttoDB(pbr_preprocessed)\n",
    "# pbr_preprocessed = missingValue_imputation(pbr_cleaned)\n",
    "# pbr_cleaned = pbr_preProcess(pbr_rawDF)\n",
    "# variable_list = set_varables(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is exported\n"
     ]
    }
   ],
   "source": [
    "# load content of the export_list into list split by new line\n",
    "# data = [line.strip() for line in open(\"exported_list.txt\", 'r')]\n",
    "with open('./exported_list.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "if (filename not in lines):\n",
    "    file_path=  os.path.join(_dir_path, filename)\n",
    "    pbr_rawDF = pd.read_csv(file_path, sep=',')\n",
    "    pbr_cleaned = pbr_preProcess(pbr_rawDF)\n",
    "    pbr_preprocessed = missingValue_imputation(pbr_cleaned)\n",
    "    try:\n",
    "        export_status = pbr_exporttoDB(pbr_preprocessed)\n",
    "        file1 = open(\"exported_list.txt\", \"a\")\n",
    "        file1.write(\"\\n\")     \n",
    "        file1.write(filename)\n",
    "        file1.close()\n",
    "        print('file is exported')\n",
    "    except Exception as e: print(e)\n",
    "\n",
    "else:\n",
    "    print('data is present in the db - check and very the trial number in tbl_pbr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Exported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = dbConn_sqlAlc()\n",
    "tbl_logdata = pd.read_sql('SELECT * FROM tbl_pbr_data where trial_no = 3', database_connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586080, 31)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_logdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586080, 29)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbr_rawDF.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c003f82c03b36fef287bdd35e08b972b8ba2e96746a620b0947100bdbd54581"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
