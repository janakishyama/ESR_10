{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "import warnings    # to avoid warning during executions\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection with mariaDB using SQL Alchemy\n",
    "def dbConn_sqlAlc():\n",
    "    database_username = 'root'\n",
    "    database_password = 'password'\n",
    "    database_ip       = '127.0.0.1:3306'\n",
    "    database_name     = 'data_dashboard'\n",
    "    database_connection = sqlalchemy.create_engine('mariadb+mariadbconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "    return database_connection\n",
    "\n",
    "# # database connection with mariaDB using Maria DB package\n",
    "def dBCon_Maria():\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                            user='root',\n",
    "                            password='password',\n",
    "                            db='data_dashboard')\n",
    "    return connection\n",
    "\n",
    "\n",
    "# connection = dBCon_Maria()\n",
    "# cursor=connection.cursor()                                            \n",
    "# database_connection = dbConn_sqlAlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "      <th>unit</th>\n",
       "      <th>species</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>remark</th>\n",
       "      <th>raw_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1629468000</td>\n",
       "      <td>1633939195</td>\n",
       "      <td>missing values from 2021-10-3 2:56:55 to 2021-...</td>\n",
       "      <td>_1_FPC13_ChCal_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1634652000</td>\n",
       "      <td>1644488995</td>\n",
       "      <td>None</td>\n",
       "      <td>_2_FPC13_ChCal_2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1625227200</td>\n",
       "      <td>1628157595</td>\n",
       "      <td>None</td>\n",
       "      <td>_3_FPC13_ThalaPs.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1614763800</td>\n",
       "      <td>1618392595</td>\n",
       "      <td>None</td>\n",
       "      <td>_4_FPC14_chamu.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1625486400</td>\n",
       "      <td>1639564200</td>\n",
       "      <td>None</td>\n",
       "      <td>_5_FPC14_RhoSa.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1618561800</td>\n",
       "      <td>1625228995</td>\n",
       "      <td>None</td>\n",
       "      <td>_6_FPC14_thalaps.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1634914800</td>\n",
       "      <td>1646645395</td>\n",
       "      <td>None</td>\n",
       "      <td>_7_FPC21_chamu.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1623051000</td>\n",
       "      <td>1626771600</td>\n",
       "      <td>None</td>\n",
       "      <td>_8_FPC21_Techu.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1620216000</td>\n",
       "      <td>1634632200</td>\n",
       "      <td>None</td>\n",
       "      <td>_9_FPC22_DiaLut.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1616427000</td>\n",
       "      <td>1626690600</td>\n",
       "      <td>None</td>\n",
       "      <td>_10_FPC23_Rhosa.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1620129600</td>\n",
       "      <td>1633964395</td>\n",
       "      <td>None</td>\n",
       "      <td>_11_FPC24_SkeMa_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1635246000</td>\n",
       "      <td>1641567595</td>\n",
       "      <td>None</td>\n",
       "      <td>_12_FPC24_SkeMa_2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1639131000</td>\n",
       "      <td>1651237195</td>\n",
       "      <td>None</td>\n",
       "      <td>_13_FPC11_Rhosa_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1634601600</td>\n",
       "      <td>1651237195</td>\n",
       "      <td>None</td>\n",
       "      <td>_14_FPC12_DiaLut.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1635244200</td>\n",
       "      <td>1651669200</td>\n",
       "      <td>None</td>\n",
       "      <td>_15_FPC22_Thalaps.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1627380000</td>\n",
       "      <td>1646827195</td>\n",
       "      <td>None</td>\n",
       "      <td>_16_FPC23_NaCho.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    instance  unit  species  start_date    end_date  \\\n",
       "0          1     6        1  1629468000  1633939195   \n",
       "1          2     6        1  1634652000  1644488995   \n",
       "2          3     6       11  1625227200  1628157595   \n",
       "3          4     1        2  1614763800  1618392595   \n",
       "4          5     1        7  1625486400  1639564200   \n",
       "5          6     1       11  1618561800  1625228995   \n",
       "6          7     4        2  1634914800  1646645395   \n",
       "7          8     4        9  1623051000  1626771600   \n",
       "8          9     3        3  1620216000  1634632200   \n",
       "9         10     5        7  1616427000  1626690600   \n",
       "10        11     7        8  1620129600  1633964395   \n",
       "11        12     7        8  1635246000  1641567595   \n",
       "12        13     8        7  1639131000  1651237195   \n",
       "13        14    10        3  1634601600  1651237195   \n",
       "14        15     3       11  1635244200  1651669200   \n",
       "15        16     5        5  1627380000  1646827195   \n",
       "\n",
       "                                               remark          raw_file_name  \n",
       "0   missing values from 2021-10-3 2:56:55 to 2021-...   _1_FPC13_ChCal_1.csv  \n",
       "1                                                None   _2_FPC13_ChCal_2.csv  \n",
       "2                                                None   _3_FPC13_ThalaPs.csv  \n",
       "3                                                None     _4_FPC14_chamu.csv  \n",
       "4                                                None     _5_FPC14_RhoSa.csv  \n",
       "5                                                None   _6_FPC14_thalaps.csv  \n",
       "6                                                None     _7_FPC21_chamu.csv  \n",
       "7                                                None     _8_FPC21_Techu.csv  \n",
       "8                                                None    _9_FPC22_DiaLut.csv  \n",
       "9                                                None    _10_FPC23_Rhosa.csv  \n",
       "10                                               None  _11_FPC24_SkeMa_1.csv  \n",
       "11                                               None  _12_FPC24_SkeMa_2.csv  \n",
       "12                                               None  _13_FPC11_Rhosa_1.csv  \n",
       "13                                               None   _14_FPC12_DiaLut.csv  \n",
       "14                                               None  _15_FPC22_Thalaps.csv  \n",
       "15                                               None    _16_FPC23_NaCho.csv  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all instances\n",
    "def list_instances():\n",
    "    database_connection = dbConn_sqlAlc()\n",
    "    tbl_lookup = pd.read_sql(\"SELECT * FROM tbl_pbr_lookup\", database_connection)\n",
    "    return tbl_lookup\n",
    "\n",
    "all_instances = list_instances()\n",
    "all_instances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to set the additional parameters\n",
    "def set_varables(trial_no):\n",
    "    database_connection = dbConn_sqlAlc()\n",
    "    tbl_lookup = pd.read_sql(\"SELECT * FROM tbl_pbr_lookup WHERE instance = ?\", database_connection, params=[trial_no])\n",
    "    species_id = int(tbl_lookup['species'])\n",
    "    trial_no = int(tbl_lookup['instance'])\n",
    "    unit_id = int(tbl_lookup['unit'])\n",
    "    start_date = int(tbl_lookup['start_date'])\n",
    "    end_date = int(tbl_lookup['end_date'])\n",
    "    parameter = 12\n",
    "    category = 6\n",
    "    return species_id, trial_no, unit_id, start_date, end_date, parameter, category\n",
    "\n",
    "\n",
    "# Function and for data extraction and preprocessing\n",
    "# Preprocessing the data setting/ adding values and data types\n",
    "def pbr_preProcess(trial_no):\n",
    "    species_id, trial_no, unit_id, start_date, end_date, parameter, category = set_varables(trial_no)\n",
    "    database_connection = dbConn_sqlAlc()\n",
    "    tbl_log_calc = pd.read_sql(\"SELECT * FROM tbl_log_data WHERE unit = ? AND parameter = ? AND category = ? AND time_epoch BETWEEN ? AND ?\", database_connection, params=[unit_id, parameter, category, start_date, end_date])\n",
    "    sum_daily = tbl_log_calc.pivot_table(index=(['date_time', 'time_epoch', 'category', 'unit', 'parameter']),columns='message',values='old_value',fill_value=0)\n",
    "    pbr_raw_tmp1 = sum_daily.reset_index()\n",
    "    pbr_raw_tmp1= pbr_raw_tmp1.drop(columns=['category', 'parameter'])\n",
    "    pbr_raw_tmp1.insert(loc=3, column='trial_no', value=trial_no)\n",
    "    pbr_raw_tmp1.insert(loc=4, column='species', value=species_id)\n",
    "    cols = list(pbr_raw_tmp1.columns)\n",
    "    a, b = cols.index('date_time'), cols.index('time_epoch')\n",
    "    cols[b], cols[a] = cols[a], cols[b]\n",
    "    pbr_raw_tmp1 = pbr_raw_tmp1[cols]      \n",
    "    return(\n",
    "    pbr_raw_tmp1\n",
    "    .rename(columns = {'time_epoch':'time_stamp', 'CD (g/l)':'D_CD_gpl', 'CO2 to algae conversion':'Co2_Conv',\n",
    "                        'Daily CO2 consumption(g)':'D_CO2_g', 'Daily CO2 consumption(g/m)':'D_CO2_gpm',\n",
    "                        'Daily CO2 conversion(g/g)':'D_Co2_Conv_gpg', 'Daily Feed(L)':'D_Feed_l', 'Daily Harvest(L)':'D_Harvest_l',\n",
    "                        'Daily Harvested algae(g)':'D_Harvest_Algae_g', 'Daily PAR(mol/m)':'D_PAR_molpm', 'Daily efficiency (%)':'D_eff_percent',\n",
    "                        'Daily productivity (g)':'D_Productivity_g', 'Daily productivity (g/m)':'D_Productivity_gpm',\n",
    "                        'PAR to algae conversion':'PAR_2Algae_Conv', 'connected reactor surface (m)':'R_Surface_m',\n",
    "                        'reactor biomassa (kg)':'R_Biomass_kg', 'reactor volume (l)':'R_Volume_l'})\n",
    "    .assign(unit = unit_id)\n",
    "    .replace('Empty', pd.np.nan)\n",
    "    .replace('inf', pd.np.nan)\n",
    "    .replace([np.inf, -np.inf], pd.np.nan)\n",
    "    .astype({'time_stamp':'int64', 'date_time':'datetime64[ns]','unit': 'int16', 'trial_no': 'int16', 'species':'int16',\n",
    "            'D_CD_gpl':'float32', 'Co2_Conv':'float32', 'D_CO2_g':'float32', 'D_CO2_gpm':'float32', 'D_Co2_Conv_gpg':'float32',\n",
    "            'D_Feed_l':'float32', 'D_Harvest_l':'float32', 'D_Harvest_Algae_g':'float32', 'D_PAR_molpm':'float32', 'D_eff_percent':'float32',\n",
    "            'D_Productivity_g':'float32', 'D_Productivity_gpm':'float32', 'PAR_2Algae_Conv':'float32', 'R_Surface_m':'float32',\n",
    "            'R_Biomass_kg':'float32', 'R_Volume_l':'float32'})\n",
    "    )\n",
    "\n",
    "# Missing Value imputation using interpolation - spline\n",
    "def missingValue_imputation(sum_hour_cleaned):\n",
    "    sum_hour_cleaned_indexed = sum_hour_cleaned.set_index('date_time')\n",
    "    null_cols = sum_hour_cleaned_indexed.columns[sum_hour_cleaned_indexed.isnull().any()].tolist()\n",
    "    if null_cols:\n",
    "        for pbr_col in null_cols:\n",
    "            sum_hour_cleaned_indexed[pbr_col]=sum_hour_cleaned_indexed[pbr_col].interpolate(option='spline')\n",
    "\n",
    "        sum_hour_cleaned_indexed = sum_hour_cleaned_indexed.reset_index(level=0)\n",
    "        new_col = list(sum_hour_cleaned.columns)\n",
    "        sum_hour_cleaned_indexed=sum_hour_cleaned_indexed[new_col]\n",
    "        return sum_hour_cleaned_indexed\n",
    "    else:\n",
    "        print('No columns with missing values')\n",
    "        return sum_hour_cleaned\n",
    "\n",
    "\n",
    "# exporting the preprocessed data into db pbr\n",
    "def pbr_exporttoDB(sumHour_preprocessed):\n",
    "    connection = dBCon_Maria()\n",
    "    cursor=connection.cursor()      \n",
    "    cols = \"`,`\".join([str(i) for i in sumHour_preprocessed.columns.tolist()])\n",
    "    for i,row in sumHour_preprocessed.iterrows():\n",
    "        sql = \"INSERT INTO `tbl_cumulative_per_day` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cursor.execute(sql, tuple(row))\n",
    "\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "#pbr_exporttoDB(cum_Dayly_cleaned)\n",
    "#cum_Dayly_cleaned = missingValue_imputation(tmp_CumDay_df)\n",
    "#tmp_CumDay_df = pbr_preProcess(trial_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ================== SET VARIABLES =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_1_FPC13_ChCal_1.csv\n"
     ]
    }
   ],
   "source": [
    "#List exported files\n",
    "with open('./exported_list.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET TRIAL NUMBER FOR SELECT THE DATA FROM LOG\n",
    "trial_no = 2\n",
    "# get file Name\n",
    "file_name = all_instances.query('instance == @trial_no')['raw_file_name'][trial_no-1]  #file_name = file_name[0]\n",
    "#file_name = file_name[trial_no-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is exported\n"
     ]
    }
   ],
   "source": [
    "# load content of the export_list into list split by new line\n",
    "# data = [line.strip() for line in open(\"exported_list.txt\", 'r')]\n",
    "with open('./exported_list.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "if (file_name not in lines):\n",
    "    tmp_CumDay_df = pbr_preProcess(trial_no)\n",
    "    cum_Dayly_cleaned = missingValue_imputation(tmp_CumDay_df)\n",
    "    try:\n",
    "        pbr_exporttoDB(cum_Dayly_cleaned)\n",
    "        file1 = open(\"exported_list.txt\", \"a\")\n",
    "        file1.write(\"\\n\")     \n",
    "        file1.write(file_name)\n",
    "        file1.close()\n",
    "        print('file is exported')\n",
    "\n",
    "        _dir_path_day = '../../../dataExport/cumulativeDATA/Day'\n",
    "        save_path = os.path.join(_dir_path_day, file_name)\n",
    "        cum_Dayly_cleaned.to_csv(save_path, index = False)\n",
    "    except Exception as e: print(e)\n",
    "\n",
    "else:\n",
    "    print('data is present in the db - check and very the trial number in tbl_pbr_data')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3e9ff110fca189b640152dbca1582818d88c9c2a9ef25df7a407523fb4d550"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
