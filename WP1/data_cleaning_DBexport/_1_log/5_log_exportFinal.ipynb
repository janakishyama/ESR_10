{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "import warnings    # to avoid warning during executions\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_2022_4_20.csv',\n",
       " 'log_2022_4_21.csv',\n",
       " 'log_2022_5_18.csv',\n",
       " 'log_2022_5_3.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all the file names into a list (files in the LOgData folder)\n",
    "os.getcwd()\n",
    "_dir_path = '../../../dataExport/LOGdata'\n",
    "raw_data = os.listdir(_dir_path)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log_2022_5_18.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================= set file name to process the log data  ===========================.\n",
    "filename = raw_data[2]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection with mariaDB using SQL Alchemy\n",
    "def dbConn_sqlAlc():\n",
    "    database_username = 'root'\n",
    "    database_password = 'password'\n",
    "    database_ip       = '127.0.0.1:3306'\n",
    "    database_name     = 'data_dashboard'\n",
    "    database_connection = sqlalchemy.create_engine('mariadb+mariadbconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "    return database_connection\n",
    "\n",
    "# # database connection with mariaDB using Maria DB package\n",
    "def dBCon_Maria():\n",
    "    connection = pymysql.connect(host='localhost',\n",
    "                            user='root',\n",
    "                            password='password',\n",
    "                            db='data_dashboard')\n",
    "    return connection\n",
    "\n",
    "\n",
    "# connection = dBCon_Maria()\n",
    "# cursor=connection.cursor()                                            \n",
    "# database_connection = dbConn_sqlAlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra non standard line - append the extra line with previous line\n",
    "def del_newline(file_path):\n",
    "    with open(file_path, 'r+', encoding=\"utf-8\") as file:\n",
    "        text = str();\n",
    "        for line in file:\n",
    "            if line[0:3] == \"202\":\n",
    "                text = text + '\\n';\n",
    "            text = text + line.strip();\n",
    "        file.seek(0);\n",
    "        file.write(text);\n",
    "\n",
    "# data type conversion and droping rows if parameter column value is missing\n",
    "def log_tweak(log_rawDF, tbl_unit):  #def log_tweak(log_rawDF, tbl_unit, tbl_parameter):\n",
    "    return (\n",
    "    log_rawDF\n",
    "    .query('Type != 1')\n",
    "    .query('Unit == (\"FPC11\", \"FPC12\", \"FPC13\", \"FPC14\", \"FPC21\", \"FPC22\", \"FPC23\", \"FPC24\", \"FFU\", \"FPP\", \"Product\", \"scheduler\")')\n",
    "    .dropna(subset=['Parameter', 'Unit'])\n",
    "    #.drop(columns=['MicroSec'])\n",
    "    .assign(Unit = log_rawDF.Unit.map(tbl_unit.set_index('unit_name')['unit_id']),\n",
    "           #Parameter = log_rawDF.Parameter.map(tbl_parameter.set_index('prm_name')['prm_id']),\n",
    "           Date_Time = pd.to_datetime(log_rawDF.Date_Time)\n",
    "           )\n",
    "    .astype({'Type' : 'int8', 'Unit' : 'int16'}) #.astype({'Type' : 'int8', 'Unit' : 'int16', 'Parameter' : 'int32'})\n",
    "    .rename(columns = {'Date_Time':'date_time', 'Time':'time_epoch', 'Type':'category', 'Unit':'unit', 'Parameter':'parameter', 'Message':'message'})\n",
    "    .fillna('empty')\n",
    "    #.sort_values('Time')\n",
    "    #.replace('/'',' ', regex=True)\n",
    "    #.info()\n",
    "    )\n",
    "\n",
    "# function to export the log data into db table - tbl_log_data\n",
    "def log_exporttoDB(log_cleaned):\n",
    "    connection = dBCon_Maria()\n",
    "    cursor=connection.cursor()      \n",
    "    cols = \"`,`\".join([str(i) for i in log_cleaned.columns.tolist()])\n",
    "    for i,row in log_cleaned.iterrows():\n",
    "        sql = \"INSERT INTO `tbl_log_data` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cursor.execute(sql, tuple(row))\n",
    "\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "# log_cleaned = log_tweak(log_rawDF, tbl_unit, tbl_parameter)\n",
    "# del_newline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is exported\n"
     ]
    }
   ],
   "source": [
    "# load content of the export_list into list split by new line\n",
    "# data = [line.strip() for line in open(\"exported_list.txt\", 'r')]\n",
    "with open('./exported_list.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "if (filename not in lines):\n",
    "    file_path=  os.path.join(_dir_path, filename)\n",
    "    #del_newline(file_path)\n",
    "    database_connection = dbConn_sqlAlc()\n",
    "    tbl_unit = pd.read_sql('SELECT unit_id, unit_name FROM tbl_unit', database_connection)\n",
    "    #tbl_parameter = pd.read_sql('SELECT prm_id, prm_name FROM tbl_log_parameters', database_connection)\n",
    "    log_rawDF = pd.read_csv(file_path, sep='*')\n",
    "    log_cleaned = log_tweak(log_rawDF, tbl_unit) #log_cleaned = log_tweak(log_rawDF, tbl_unit, tbl_parameter)\n",
    "    try:\n",
    "        export_status = log_exporttoDB(log_cleaned)\n",
    "        file1 = open(\"exported_list.txt\", \"a+\")\n",
    "        file1.write(\"\\n\")       \n",
    "        file1.write(filename)\n",
    "        file1.close()\n",
    "        print('file is exported')\n",
    "    except Exception as e: print(e)\n",
    "\n",
    "else:\n",
    "    print('data is present in the db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3e9ff110fca189b640152dbca1582818d88c9c2a9ef25df7a407523fb4d550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
